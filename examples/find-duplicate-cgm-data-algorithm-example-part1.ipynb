{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The purpose of this algorithm is to find duplicate records that have different UTC time stamps between different uploads. It does NOT consider the case where there is duplicate data within an upload.\n",
    "\n",
    "This algorithm is also NOT designed to handle the case where duplicate data has the same time stamp, as there are more efficient functions  (e.g., the duplicated function in pandas) that can quickly find and delete duplicated data. In fact, one should get rid of those types of duplicates before running this algorithm.  \n",
    "\n",
    "Also, there are two key steps in the de-duplication process: 1) identifying duplicates, and 2) getting rid of the bad duplicate(s). This algorithm only addresses (1); however, if we can find all cases of duplicates from step (1) it should allow us to examine and solve (2).\n",
    "\n",
    "This method is comprehensive in the sense that it looks for duplicates between every unique pair-wise combination of uploadIds for a given user. While this may be overkill, as it may compare data that is separted by several years, it is important that we don't make any assumptions about the actual time of the data, as the whole point of the algorithm is to find duplicates where the times are different. Further, we have seen a few cases where the UTC time can be off by months or years.\n",
    "\n",
    "## Algorithm Logic\n",
    "\n",
    "This algorithm finds sequences of cgm values that are duplicated between two different uploadIds. Here is the basic logic of the algorithm. For one user's data, we loop through each unique pair-wise combination of uploadIds. We do the following to each uploadId:\n",
    "* First we round all of the cgm data to the nearest 5 minutes.\n",
    "* Second we create a contiguous time series between the first data point and the last data point in the time series.\n",
    "* We then merge the two time series, so that the missing cgm points are filled with nans.\n",
    "\n",
    "We then repeat for the following for each unique pairwise combination of uploadId:\n",
    "* We take the longer/larger time series and call it TL, and the shorter one Ts, which is useful for keeping track of which indices match.\n",
    "* At this step, there is an optional preprocessing step that orders the shift indices to speed up the algorithm (see details in the second read example below).\n",
    "* Next we shift Ts over TL, and at each step we calculate the element by element difference between cgm values. If there is an exact match, then the difference will be zero.\n",
    "* At each step we count the number of zeros, and if it exceeds an algorithm defined threshold, we tag the sequence in both time series as being duplicate. \n",
    "\n",
    "For the examples below we will focus on cgm time series data, but if there are other data types that are missing deviceTime data and/or have incorrect UTC times, this algorithm can be adapted to those situations too. The following examples are given below:\n",
    "* a very simple (fake) illustrative example\n",
    "* an example with real Tidepool donor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple (fake) illustrative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying index -4\n",
      "[150]\n",
      "[220.]\n",
      "difference =  [-70.]\n",
      "number of zeros =  0\n",
      "trying index -3\n",
      "[150 160]\n",
      "[210. 220.]\n",
      "difference =  [-60. -60.]\n",
      "number of zeros =  0\n",
      "trying index -2\n",
      "[150 160 170]\n",
      "[200. 210. 220.]\n",
      "difference =  [-50. -50. -50.]\n",
      "number of zeros =  0\n",
      "trying index -1\n",
      "[150 160 170 180]\n",
      "[ nan 200. 210. 220.]\n",
      "difference =  [ nan -40. -40. -40.]\n",
      "number of zeros =  0\n",
      "trying index 0\n",
      "[150 160 170 180 190]\n",
      "[180.  nan 200. 210. 220.]\n",
      "difference =  [-30.  nan -30. -30. -30.]\n",
      "number of zeros =  0\n",
      "trying index 1\n",
      "[160 170 180 190 200]\n",
      "[180.  nan 200. 210. 220.]\n",
      "difference =  [-20.  nan -20. -20. -20.]\n",
      "number of zeros =  0\n",
      "trying index 2\n",
      "[170 180 190 200 210]\n",
      "[180.  nan 200. 210. 220.]\n",
      "difference =  [-10.  nan -10. -10. -10.]\n",
      "number of zeros =  0\n",
      "trying index 3\n",
      "[180 190 200 210 220]\n",
      "[180.  nan 200. 210. 220.]\n",
      "difference =  [ 0. nan  0.  0.  0.]\n",
      "number of zeros =  4\n",
      "FOUND DUPLICATES AT 3\n",
      "trying index 4\n",
      "[190 200 210 220 230]\n",
      "[180.  nan 200. 210. 220.]\n",
      "difference =  [10. nan 10. 10. 10.]\n",
      "number of zeros =  0\n",
      "trying index 5\n",
      "[200 210 220 230 240]\n",
      "[180.  nan 200. 210. 220.]\n",
      "difference =  [20. nan 20. 20. 20.]\n",
      "number of zeros =  0\n",
      "trying index 6\n",
      "[210 220 230 240]\n",
      "[ nan 200. 210. 220.]\n",
      "difference =  [nan 20. 20. 20.]\n",
      "number of zeros =  0\n",
      "trying index 7\n",
      "[220 230 240]\n",
      "[200. 210. 220.]\n",
      "difference =  [20. 20. 20.]\n",
      "number of zeros =  0\n",
      "trying index 8\n",
      "[230 240]\n",
      "[210. 220.]\n",
      "difference =  [20. 20.]\n",
      "number of zeros =  0\n",
      "trying index 9\n",
      "[240]\n",
      "[220.]\n",
      "difference =  [20.]\n",
      "number of zeros =  0\n"
     ]
    }
   ],
   "source": [
    "# here is the setup for this example\n",
    "TL = np.array([150, 160, 170, 180, 190, 200, 210, 220, 230, 240])\n",
    "Ts = np.array([180, np.nan, 200, 210, 220])\n",
    "minThreshold = 3  # NOTE: a real example should consider a much \n",
    "# higher threshold, like 48, 96, or 288 \n",
    "\n",
    "for i in range(-len(Ts) + 1, len(TL)):\n",
    "    print(\"trying index\", i)\n",
    "    tempTL = TL[max([0, i]):min([len(TL), (len(Ts) + i)])]\n",
    "    print(tempTL)\n",
    "    tempTs = Ts[-len(tempTL):]\n",
    "    print(tempTs)\n",
    "    tempDiff = tempTL - tempTs\n",
    "    print(\"difference = \", tempDiff)\n",
    "    nZeros = sum(tempDiff == 0)\n",
    "    print(\"number of zeros = \", nZeros)\n",
    "    if nZeros >= minThreshold:\n",
    "        print(\"FOUND DUPLICATES AT\", i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real world example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COMING SOON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
