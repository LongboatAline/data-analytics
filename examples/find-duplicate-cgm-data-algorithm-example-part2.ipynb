{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The purpose of this algorithm is to find duplicate records that have different UTC time stamps between different uploads. It does NOT consider the case where there is duplicate data within an upload.\n",
    "\n",
    "This algorithm is also NOT designed to handle the case where duplicate data has the same time stamp, as there are more efficient functions  (e.g., the duplicated function in pandas) that can quickly find and delete duplicated data. In fact, one should get rid of those types of duplicates before running this algorithm.  \n",
    "\n",
    "Also, there are two key steps in the de-duplication process: 1) identifying duplicates, and 2) getting rid of the bad duplicate(s). This algorithm only addresses (1); however, if we can find all cases of duplicates from step (1) it should allow us to examine and solve (2).\n",
    "\n",
    "This method is comprehensive in the sense that it looks for duplicates between every unique pair-wise combination of uploadIds for a given user. While this may be overkill, as it may compare data that is separted by several years, it is important that we don't make any assumptions about the actual time of the data, as the whole point of the algorithm is to find duplicates where the times are different. Further, we have seen a few cases where the UTC time can be off by months or years.\n",
    "\n",
    "## Algorithm Logic\n",
    "\n",
    "This algorithm finds sequences of cgm values that are duplicated between two different uploadIds. Here is the basic logic of the algorithm. For one user's data, we loop through each unique pair-wise combination of uploadIds. We do the following to each uploadId:\n",
    "* First we round all of the cgm data to the nearest 5 minutes.\n",
    "* Second we create a contiguous time series between the first data point and the last data point in the time series.\n",
    "* We then merge the two time series, so that the missing cgm points are filled with nans.\n",
    "\n",
    "We then repeat for the following for each unique pairwise combination of uploadId:\n",
    "* We take the longer/larger time series and call it TL, and the shorter one Ts, which is useful for keeping track of which indices match.\n",
    "* At this step, there is an optional preprocessing step that orders the shift indices to speed up the algorithm (see details in the second read example below).\n",
    "* Next we shift Ts over TL, and at each step we calculate the element by element difference between cgm values. If there is an exact match, then the difference will be zero.\n",
    "* At each step we count the number of zeros, and if it exceeds an algorithm defined threshold, we tag the sequence in both time series as being duplicate. \n",
    "\n",
    "For the examples below we will focus on cgm time series data, but if there are other data types that are missing deviceTime data and/or have incorrect UTC times, this algorithm can be adapted to those situations too. The following examples are given below:\n",
    "* a very simple (fake) illustrative example\n",
    "* an example with real Tidepool donor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import correlate\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real world example\n",
    "for this example we will look at 1 of the 305 unique combinations,\n",
    "for one donor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the minimum number of duplicate data points that are\n",
    "# required to flag the duplicates \n",
    "minThreshold = 96  # 288\n",
    "\n",
    "# load in data\n",
    "hashID = \"0289cfb8bd6d61ccf1f31c07aa146b7b14f0eb74474be4311860d9d77dd30f15\"\n",
    "dataPath = os.path.join(\".\", \"data\")\n",
    "data = pd.read_csv(os.path.join(dataPath, hashID + \".csv\"), low_memory=False)\n",
    "\n",
    "# this is a great example, becuase there are missing data points in both\n",
    "# time series\n",
    "uploadId_A = \"upid_ff6bf4b6fde9c9bc45bb211de131d225\"\n",
    "uploadId_B = \"upid_12164f5817e09ab7bffb439d8c260131\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cIndex = 0\n",
    "results = pd.DataFrame(columns=[\"uploadId_A\", \"span_uploadId_A\",\n",
    "                                \"n_uploadId_A\",\n",
    "                                \"uploadId_B\", \"span_uploadId_B\",\n",
    "                                \"n_uploadId_B\", \"elapsedTime\",\n",
    "                                \"correlationOfIndex\", \"nDuplicates\",\n",
    "                                \"averageTimeDifference\",\n",
    "                                \"startIndex_A\", \"endIndex_A\",\n",
    "                                \"startIndex_B\", \"endIndex_B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for the algorithm\n",
    "\n",
    "# work with just the cgm data (though this can be modified for pump data)\n",
    "cgm = data.loc[data.type == \"cbg\", [\"deviceTime\", \"time\", \"value\", \"uploadId\"]]\n",
    "\n",
    "# convert mmol/L to mg/dL\n",
    "cgm = cgm.rename(columns={\"value\": \"mmol_L\"})\n",
    "cgm[\"mg_dL\"] = (cgm[\"mmol_L\"] * 18.01559).astype(int)\n",
    "\n",
    "# round utc time to the nearest 30 seconds, and then to nearest 5 minutes\n",
    "ns5min=5*60*1E9\n",
    "ns30sec=0.5*60*1E9\n",
    "cgm[\"roundedTime30sec\"] = pd.to_datetime(cgm[\"time\"]).dt.round(\"30S\")\n",
    "#    pd.to_datetime((pd.to_datetime(cgm.time).astype(np.int64) // ns30sec) * ns30sec)\n",
    "\n",
    "cgm[\"roundedTime5min\"] = \\\n",
    "pd.to_datetime((pd.to_datetime(cgm[\"roundedTime30sec\"]).astype(np.int64) // ns5min + 1) * ns5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data for just the two uploads\n",
    "cgm_A = cgm[cgm.uploadId == uploadId_A].reset_index().rename(columns={\"index\":\"originalIndex\"})\n",
    "cgm_B = cgm[cgm.uploadId == uploadId_B].reset_index().rename(columns={\"index\":\"originalIndex\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration0 = pd.to_datetime(cgm[cgm.uploadId == uploadId_A].time.max()) - \\\n",
    "    pd.to_datetime(cgm[cgm.uploadId == uploadId_A].time.min())\n",
    "duration1 = pd.to_datetime(cgm[cgm.uploadId == uploadId_B].time.max()) - \\\n",
    "    pd.to_datetime(cgm[cgm.uploadId == uploadId_B].time.min())\n",
    "\n",
    "if duration0 >= duration1:\n",
    "    results.loc[cIndex, [\"uploadId_A\"]] = uploadId_A\n",
    "    results.loc[cIndex, [\"span_uploadId_A\"]] = duration0\n",
    "    results.loc[cIndex, [\"n_uploadId_A\"]] = len(cgm_A)\n",
    "    results.loc[cIndex, [\"uploadId_B\"]] = uploadId_B\n",
    "    results.loc[cIndex, [\"span_uploadId_B\"]] = duration1\n",
    "    results.loc[cIndex, [\"n_uploadId_B\"]] = len(cgm_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a continguous time series from the first to last data point\n",
    "contiguousBeginDateTime_A = min(cgm_A.roundedTime5min)\n",
    "contiguousEndDateTime_A = max(cgm_A.roundedTime5min)\n",
    "rng_A = pd.date_range(contiguousBeginDateTime_A, contiguousEndDateTime_A, freq=\"5min\")\n",
    "contiguousData_A = pd.DataFrame(rng_A, columns=[\"dateTime\"])\n",
    "\n",
    "# merge data\n",
    "contiguousData_A = pd.merge(contiguousData_A, cgm_A,\n",
    "                            left_on=\"dateTime\", right_on=\"roundedTime5min\",\n",
    "                            how=\"left\")\n",
    "\n",
    "contiguousBeginDateTime_B = min(cgm_B.roundedTime5min)\n",
    "contiguousEndDateTime_B = max(cgm_B.roundedTime5min)\n",
    "rng_B = pd.date_range(contiguousBeginDateTime_B, contiguousEndDateTime_B, freq=\"5min\")\n",
    "contiguousData_B = pd.DataFrame(rng_B, columns=[\"dateTime\"])\n",
    "\n",
    "# merge data\n",
    "contiguousData_B = pd.merge(contiguousData_B, cgm_B,\n",
    "                            left_on=\"dateTime\", right_on=\"roundedTime5min\",\n",
    "                            how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build faster indexing with cross correlation here\n",
    "\n",
    "# 1. figure out the median of TL and Ts combined (T_all), so that \n",
    "# data can be separated into an equal number of positive and negative\n",
    "# values.\n",
    "\n",
    "T_all = pd.concat([contiguousData_A.mg_dL, contiguousData_B.mg_dL])\n",
    "median_all = T_all.median()\n",
    "min_all = T_all.min()\n",
    "max_all = T_all.max()\n",
    "\n",
    "# scale TL and Ts data so that the following three things happen:\n",
    "# 1. median_all <= value <= max_all gets normalized between 1 and 1.1\n",
    "# 2. min_all <= value < median_all gets normalized between -1.1 and -1 \n",
    "# 3. all nans get replaced with zeros\n",
    "def normalize(value, valMin, valMax, normMin, normMax):\n",
    "    a = (normMax - normMin)/(valMax - valMin)\n",
    "    b = normMax - a * valMax\n",
    "    newvalue = a * value + b\n",
    "    \n",
    "    return newvalue\n",
    "\n",
    "# TODO: turn this into a function\n",
    "TL1 = normalize(contiguousData_A.mg_dL[contiguousData_A.mg_dL >= median_all],\n",
    "                median_all, max_all, 1, 1.1)\n",
    "\n",
    "TL2 = normalize(contiguousData_A.mg_dL[contiguousData_A.mg_dL < median_all],\n",
    "                min_all, median_all, -1.1, -1)\n",
    "\n",
    "TL3 = contiguousData_A.mg_dL[contiguousData_A.mg_dL.isna()].fillna(0)\n",
    "\n",
    "TL_norm = pd.concat([TL1, TL2, TL3]).sort_index() \n",
    "\n",
    "Ts1 = normalize(contiguousData_B.mg_dL[contiguousData_B.mg_dL >= median_all],\n",
    "                median_all, max_all, 1, 1.1)\n",
    "\n",
    "Ts2 = normalize(contiguousData_B.mg_dL[contiguousData_B.mg_dL < median_all],\n",
    "                min_all, median_all, -1.1, -1)\n",
    "\n",
    "Ts3 = contiguousData_B.mg_dL[contiguousData_B.mg_dL.isna()].fillna(0)\n",
    "\n",
    "Ts_norm = pd.concat([Ts1, Ts2, Ts3]).sort_index() \n",
    "\n",
    "# do the cross correlation to get the most likely indices that contain duplicates\n",
    "xCorr = pd.Series(correlate(TL_norm, Ts_norm), name=\"value\")\n",
    "indicesToTry = pd.DataFrame(xCorr[xCorr > 1])\n",
    "indicesToTry[\"TLindex\"] = indicesToTry.index + 1 - len(contiguousData_B)\n",
    "indicesToTry = indicesToTry.sort_values(by=\"value\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying index 181\n",
      "found it at index 181\n",
      "finished and it took 0.049 secs\n"
     ]
    }
   ],
   "source": [
    "TL = np.array(contiguousData_A.mg_dL)\n",
    "Ts = np.array(contiguousData_B.mg_dL)\n",
    "t = time.time()\n",
    "\n",
    "for i in indicesToTry[\"TLindex\"]:\n",
    "    print(\"trying index\", i)\n",
    "    tempTL = TL[max([0, i]):min([len(TL), (len(Ts) + i)])]\n",
    "    tempDiff = tempTL - Ts[-len(tempTL):]\n",
    "    nZeros = sum(tempDiff == 0)\n",
    "    if nZeros >= minThreshold:\n",
    "        print(\"found it at index\", i)\n",
    "        results.loc[cIndex, [\"correlationOfIndex\"]] = round(indicesToTry[indicesToTry[\"TLindex\"] == i].value.values[0],1)\n",
    "        dupTL = contiguousData_A[max([0, i]):min([len(TL), (len(Ts) + i)])]\n",
    "        dupTs = contiguousData_B[-len(tempTL):]\n",
    "        combined = pd.concat([dupTL.reset_index(drop=True).add_suffix(\".TL\"),\n",
    "                              dupTs.reset_index(drop=True).add_suffix(\".Ts\")], axis=1)\n",
    "\n",
    "        results.loc[cIndex, [\"nDuplicates\"]] = nZeros\n",
    "        results.loc[cIndex, [\"startIndex_A\"]] = \\\n",
    "            combined.loc[combined[\"mg_dL.TL\"].notnull(), \"originalIndex.TL\"].min()\n",
    "        results.loc[cIndex, [\"endIndex_A\"]] = \\\n",
    "            combined.loc[combined[\"mg_dL.TL\"].notnull(), \"originalIndex.TL\"].max()\n",
    "\n",
    "        results.loc[cIndex, [\"startIndex_B\"]] = \\\n",
    "            combined.loc[combined[\"mg_dL.Ts\"].notnull(), \"originalIndex.Ts\"].min()\n",
    "        results.loc[cIndex, [\"endIndex_B\"]] = \\\n",
    "            combined.loc[combined[\"mg_dL.Ts\"].notnull(), \"originalIndex.Ts\"].max()\n",
    "\n",
    "        cTimeDifference = pd.to_datetime(combined[\"time.TL\"]) - \\\n",
    "                            pd.to_datetime(combined[\"time.Ts\"])\n",
    "\n",
    "        averageTimeDifference = cTimeDifference.dt.seconds.mean()\n",
    "        results.loc[cIndex, [\"averageTimeDifference\"]] = averageTimeDifference\n",
    "\n",
    "        break\n",
    "\n",
    "elapsedTime = time.time() - t\n",
    "results.loc[cIndex, [\"elapsedTime\"]] = elapsedTime\n",
    "print(\"finished and it took\", round(elapsedTime, 3), \"secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>uploadId_A</th>\n",
       "      <td>upid_ff6bf4b6fde9c9bc45bb211de131d225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>span_uploadId_A</th>\n",
       "      <td>30 days 21:43:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_uploadId_A</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uploadId_B</th>\n",
       "      <td>upid_12164f5817e09ab7bffb439d8c260131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>span_uploadId_B</th>\n",
       "      <td>29 days 08:38:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_uploadId_B</th>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elapsedTime</th>\n",
       "      <td>0.0487611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correlationOfIndex</th>\n",
       "      <td>326.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nDuplicates</th>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>averageTimeDifference</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startIndex_A</th>\n",
       "      <td>29090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>endIndex_A</th>\n",
       "      <td>30212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startIndex_B</th>\n",
       "      <td>29091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>endIndex_B</th>\n",
       "      <td>30213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0\n",
       "uploadId_A             upid_ff6bf4b6fde9c9bc45bb211de131d225\n",
       "span_uploadId_A                             30 days 21:43:35\n",
       "n_uploadId_A                                            1000\n",
       "uploadId_B             upid_12164f5817e09ab7bffb439d8c260131\n",
       "span_uploadId_B                             29 days 08:38:38\n",
       "n_uploadId_B                                             288\n",
       "elapsedTime                                        0.0487611\n",
       "correlationOfIndex                                     326.2\n",
       "nDuplicates                                              288\n",
       "averageTimeDifference                                      0\n",
       "startIndex_A                                           29090\n",
       "endIndex_A                                             30212\n",
       "startIndex_B                                           29091\n",
       "endIndex_B                                             30213"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
